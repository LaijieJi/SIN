{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron applied to the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the dataset:** $\\;$ we also check that the data matrix and labels have the right number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150, 1) \n",
      " [[5.1015625  3.5        1.40039062 0.19995117 0.        ]\n",
      " [4.8984375  3.         1.40039062 0.19995117 0.        ]\n",
      " [4.69921875 3.19921875 1.29980469 0.19995117 0.        ]\n",
      " [4.6015625  3.09960938 1.5        0.19995117 0.        ]\n",
      " [5.         3.59960938 1.40039062 0.19995117 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; from sklearn.datasets import load_iris\n",
    "iris = load_iris(); X = iris.data.astype(np.float16);\n",
    "y = iris.target.astype(np.uint).reshape(-1, 1);\n",
    "print(X.shape, y.shape, \"\\n\", np.hstack([X, y])[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset partition:** $\\;$ We create a split of the Iris dataset with $20\\%$ of data for test and the rest for training, previously shuffling the data according to a given seed provided by a random number generator. Here, as in all code that includes randomness (which requires generating random numbers), it is convenient to fix said seed to be able to reproduce experiments with accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron implementation:** $\\;$ returns weights in homogeneous notation, $\\mathbf{W}\\in\\mathbb{R}^{(1+D)\\times C};\\;$ also the number of errors and iterations executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(X, y, b=0.1, a=1.0, K=200):\n",
    "    N, D = X.shape; Y = np.unique(y); C = Y.size; W = np.zeros((1+D, C))\n",
    "    for k in range(1, K+1):\n",
    "        E = 0\n",
    "        for n in range(N):\n",
    "            xn = np.array([1, *X[n, :]])\n",
    "            cn = np.squeeze(np.where(Y==y[n]))\n",
    "            gn = W[:,cn].T @ xn; err = False\n",
    "            for c in np.arange(C):\n",
    "                if c != cn and W[:,c].T @ xn + b >= gn:\n",
    "                    W[:, c] = W[:, c] - a*xn; err = True\n",
    "            if err:\n",
    "                W[:, cn] = W[:, cn] + a*xn; E = E + 1\n",
    "        if E == 0:\n",
    "            break;\n",
    "    return W, E, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning a (linear) classifier with Perceptron:** $\\;$ Perceptron minimizes the number of training errors (with margin $b$)\n",
    "$$\\mathbf{W}^*=\\operatorname*{argmin}_{\\mathbf{W}=(\\boldsymbol{w}_1,\\dotsc,\\boldsymbol{w}_C)}\\sum_n\\;\\mathbb{ I}\\biggl(\\max_{c\\neq y_n}\\;\\boldsymbol{w}_c^t\\boldsymbol{x}_n+b \\;>\\; \\boldsymbol{w}_{y_n}^t\\boldsymbol{ x}_n\\biggr)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations executed:  200\n",
      "Number of training errors:  2\n",
      "Weight vectors of the classes (in columns and with homogeneous notation):\n",
      " [[  10.           85.         -142.        ]\n",
      " [ -49.421875    -68.19140625 -176.47265625]\n",
      " [  50.171875     -1.72460938 -181.06445312]\n",
      " [-189.91210938  -87.70507812   68.69726562]\n",
      " [ -86.40258789 -137.78149414  157.88415527]]\n"
     ]
    }
   ],
   "source": [
    "W, E, k = perceptron(X_train, y_train)\n",
    "print(\"Number of iterations executed: \", k)\n",
    "print(\"Number of training errors: \", E)\n",
    "print(\"Weight vectors of the classes (in columns and with homogeneous notation):\\n\", W);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculation of test error rate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate on test: 16.7%\n"
     ]
    }
   ],
   "source": [
    "X_testh = np.hstack([np.ones((len(X_test), 1)), X_test])\n",
    "y_test_pred  = np.argmax(X_testh @ W, axis=1).reshape(-1, 1)\n",
    "err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "print(f\"Error rate on test: {err_test:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Margin adjustment:** $\\;$ experiment to learn a value of $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3 1000\n",
      "0.01 5 1000\n",
      "0.1 3 1000\n",
      "10 6 1000\n",
      "100 6 1000\n"
     ]
    }
   ],
   "source": [
    "for b in (.0, .01, .1, 10, 100):\n",
    "    W, E, k = perceptron(X_train, y_train, b=b, K=1000)\n",
    "    print(b, E, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of results:** $\\;$ the training data does not appear to be linearly separable; it is not clear that a margin greater than zero can improve results, especially since we only have $30$ test samples; with a margin $b=0.1$ we have already seen that an error (in test) of $16.7\\%$ is obtained"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
