{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U2.4 Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** $\\;$ $\\;$ Let be a logistic regression model for a classification problem in $C=3$ classes and data represented by vectors of dimension $D=2$\n",
    "$$p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})=\\operatorname{Cat}(y\\mid\\mathcal{S}(\\mathbf{W}^t\\boldsymbol{x}+\\boldsymbol{b}))\n",
    "\\quad\\text{with}\\quad%\n",
    "\\mathbf{W}^t=\\begin{pmatrix}1&1\\\\-1&1\\\\0&0\\end{pmatrix}\\in\\mathbb{R}^{C\\times D}%\n",
    "\\quad\\text{and}\\quad%\n",
    "\\boldsymbol{b}=\\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}$$\n",
    "The probability $P$ that $\\boldsymbol{x}=(0.5, 0.5)^t$ belongs to class $1$ is:\n",
    "1. $P < 0.25$\n",
    "2. $0.25\\leq P < 0.5$\n",
    "3. $0.5\\leq P < 0.75$\n",
    "4. $0.75\\leq P$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** $\\;$ Option 3\n",
    "$$p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})=\\mathcal{S}\\left(\\begin{pmatrix}1&1\\\\-1&1\\\\0&0\\end{pmatrix}\\begin{pmatrix}0.5\\\\0.5\\end{pmatrix}\\right)_1=\\mathcal{S}((1, 0, 0)^t)_1=\\dfrac{e}{e+2}=\\dfrac{1}{1+2/e}=0.5761$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** $\\;$ Let be a logistic regression model in compact (homogeneous) notation for a classification problem in $C=3$ classes and data represented by vectors of dimension $D=3$\n",
    "$$p(\\boldsymbol{y}\\mid\\boldsymbol{x};\\mathbf{W})=\\operatorname{Cat}(\\boldsymbol{y}\\mid \\mathcal{S}(\\mathbf{W}^t \\boldsymbol{x}))\n",
    "\\quad\\text{with}\\quad%\n",
    "\\mathbf{W}^t=\\begin{pmatrix}1&-1&0\\\\0&1&0\\\\1&-1&1\\end{pmatrix}\\in\\mathbb{R}^{C\\times D}$$\n",
    "Update the value of $\\mathbf{W}$ by one gradient descent iteration with training set $\\,\\mathcal{D}=\\{(\\boldsymbol{x}=(1, 1, 1)^t , y=1)\\}\\,$ and learning factor $\\,\\eta=0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{a}%\n",
    "&=\\mathbf{W}^t\\boldsymbol{x}%\n",
    "=\\begin{pmatrix}1&-1&0\\\\0&1&0\\\\1&-1&1\\end{pmatrix}\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}%\n",
    "=\\begin{pmatrix}0\\\\1\\\\1\\end{pmatrix}\\\\[3mm]%\n",
    "\\boldsymbol{\\mu}%\n",
    "&=S(\\boldsymbol{a})%\n",
    "=\\dfrac{1}{1+2e}\\begin{pmatrix}1\\\\e\\\\e\\end{pmatrix}%\n",
    "=\\begin{pmatrix}0.1554\\\\0.4223\\\\0.4223\\end{pmatrix}\\\\[3mm]%\n",
    "\\mathbf{W}%\n",
    "&=\\mathbf{W}-\\eta\\,\\boldsymbol{x}(\\boldsymbol{\\mu}-\\boldsymbol{y})^t\\\\%\n",
    "&=\\begin{pmatrix}1&0&1\\\\-1&1&-1\\\\0&0&1\\end{pmatrix}-0.1\\,\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}(-0.8446, 0.4223, 0.4223)\\\\%\n",
    "&=\\begin{pmatrix}1&0&1\\\\-1&1&-1\\\\0&0&1\\end{pmatrix}-\\begin{pmatrix}-0.0845&0.0422&0.0422\\\\-0.0845&0.0422&0.0422\\\\-0.0845&0.0422&0.0422\\end{pmatrix}\\\\%\n",
    "&=\\begin{pmatrix}1.0845&-0.0422&0.9578\\\\-0.9155&0.9578&-1.0422\\\\0.0845&-0.0422&0.9578\\end{pmatrix}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** The following table shows a training set of $2$ samples with $2$ dimensions that belong to $2$ classes:\n",
    "\n",
    "<center>\n",
    "\n",
    "|$n$|$x_{n1}$|$x_{n2}$|$c_n$|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|1|1|0|1|\n",
    "|2|1|1|2|\n",
    "\n",
    "</center>\n",
    "\n",
    "In addition, the following table represents an initial weight matrix with the weights of each class per columns:\n",
    "\n",
    "<center>\n",
    "\n",
    "|$w_1$|$w_2$|\n",
    "|:-:|:-:|\n",
    "|0|0|\n",
    "|0|0|\n",
    "|-0.25|0.25|\n",
    "\n",
    "</center>\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. Compute the vector of logits for each training sample.\n",
    "2. Apply the softmax function to the vector of logits for each training sample.\n",
    "3. Classify every training sample. In case of a tie, choose any class.\n",
    "4. Compute the gradient of the function NLL at the point of the initial weight matrix.\n",
    "5. Update the initial weight matrix applying gradient descent with learning rate $\\eta=1.0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soluci√≥n\n",
    "\n",
    "0. First, we must mind that the training samples are provided by rows. Then, we have to prepare the training samples to be ready to be applied in logistic regression. To this purpsoe, we convert training sample into homogeneous notation and class labels into one-hot encoding:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\boldsymbol{x_1} = (1,1,0)^t; \\quad \\boldsymbol{y_1} = (1,0)^t; \\\\[0.25cm]\n",
    "&\\boldsymbol{x_2} = (1,1,1)^t; \\quad \\boldsymbol{y_2} = (0,1)^t; \\\\[0.25cm]\n",
    "\n",
    "&\\mathbf{W}_0 = \\begin{pmatrix}\n",
    "0 & 0\\\\\n",
    "0 & 0\\\\\n",
    "-0.25 & 0.25\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "1. Vector of logits for each training sample, that is, $\\boldsymbol{a_1}$ and $\\boldsymbol{a_2}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{a_1} &= \\mathbf{W}_0^t \\boldsymbol{x_1} = \\begin{pmatrix} 0 & 0  & -0.25\\\\ 0 & 0  & 0.25 \\end{pmatrix} \\cdot \\begin{pmatrix} 1\\\\ 1\\\\ 0\\end{pmatrix} = (0,0)^t\\\\\n",
    "\n",
    "\\boldsymbol{a_2} &= \\mathbf{W}_0^t \\boldsymbol{x_2} = \\begin{pmatrix} 0 & 0  & -0.25\\\\ 0 & 0  & 0.25 \\end{pmatrix} \\cdot \\begin{pmatrix} 1\\\\ 1\\\\ 1\\end{pmatrix} = (-0.25,0.25)^t\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "2. Applying the softmax function to obtain $\\boldsymbol{\\mu}_1$ and $\\boldsymbol{\\mu}_2$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{\\mu}_1 &= \\mathcal{S}(\\boldsymbol{a_1}) = \\left(\\frac{e^{0}}{e^0+e^0}, \\frac{e^{0}}{e^0+e^0}\\right)^t = (0.5, 0.5)^t \\\\\n",
    "\\boldsymbol{\\mu}_2 &= \\mathcal{S}(\\boldsymbol{a_2}) = \\left(\\frac{e^{-0.25}}{e^{-0.25}+e^{0.25}}, \\frac{e^{0.25}}{e^{-0.25}+e^{0.25}}\\right)^t = (0.38, 0.62)^t \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "3. Classification of $\\boldsymbol{x_1}$ y $\\boldsymbol{x_2}$:\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\hat{c}(\\boldsymbol{x_1}) &= \\operatorname*{argmax}\\limits_{c} \\; \\mu_{1c} = \\operatorname*{argmax}\\limits_{c} \\; [0.5, 0.5] = 1 \\\\\n",
    "\n",
    "\\hat{c}(\\boldsymbol{x_2}) &= \\operatorname*{argmax}\\limits_{c} \\; \\mu_{2c} = \\operatorname*{argmax}\\limits_{c} \\; [0.38, 0.62] = 2 \\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "4. Computing the gradient of the NLL function at the point of the initial weight matrix:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial\\operatorname{NLL}}{\\partial\\mathbf{W}^t}\\biggr\\vert_{\\mathbf{W}_0} &= \\frac{1}{N}\\sum_{n=1}^N\\boldsymbol{x}_n(\\boldsymbol{\\mu}_n-\\boldsymbol{y}_n)^t \\\\\n",
    "                &= \\frac{1}{2} \\cdot \\left(\\boldsymbol{x}_1(\\boldsymbol{\\mu}_1-\\boldsymbol{y}_1)^t + \\boldsymbol{x}_2(\\boldsymbol{\\mu}_2-\\boldsymbol{y}_2)^t \\right) \\\\\n",
    "                &= \\frac{1}{2} \\cdot \\left( \\begin{pmatrix} 1\\\\ 1\\\\ 0\\end{pmatrix} \\cdot (-0.5, 0.5) + \\begin{pmatrix} 1\\\\ 1\\\\ 1\\end{pmatrix} \\cdot (0.38, -0.38)  \\right) \\\\\n",
    "                &= \\frac{1}{2} \\cdot \\left( \\begin{pmatrix} -0.5 & 0.5\\\\ -0.5 & 0.5\\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0.38 & -0.38\\\\ 0.38 & -0.38\\\\ 0.38 & -0.38 \\end{pmatrix} \\right) \\\\\n",
    "                &= \\frac{1}{2} \\cdot \\begin{pmatrix} -0.12 & 0.12\\\\ -0.12 & 0.12\\\\ 0.38 & -0.38 \\end{pmatrix} \\\\\n",
    "                &= \\begin{pmatrix} -0.06 & 0.06\\\\ -0.06 & 0.06\\\\ 0.19 & -0.19 \\end{pmatrix} \\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "5. Updated the weight matrix:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\\mathbf{W}_1^t &= \\mathbf{W}_0^t - \\eta \\frac{\\partial\\operatorname{NLL}}{\\partial\\mathbf{W}^t}\\biggr\\vert_{\\mathbf{W}_0} \\\\ \n",
    "               &= \\begin{pmatrix} 0 & 0  \\\\ 0 & 0 \\\\ -0.25 & 0.25 \\end{pmatrix} - 1 \\cdot \\begin{pmatrix} -0.06 & 0.06\\\\ -0.06 & 0.06\\\\ 0.19 & -0.19 \\end{pmatrix} \\\\\n",
    "               &= \\begin{pmatrix} 0.06 & -0.06  \\\\ 0.06 & -0.06 \\\\ -0.44 & 0.44 \\end{pmatrix}\n",
    "\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
